{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인공신경망 기초: 퍼셉트론(Perceptron)의 이해 \n",
    "⁻   인공신경망의 역사, 특징\n",
    "⁻   퍼셉트론의 구조 및a 동작 \n",
    "⁻   퍼셉트론의 구현\n",
    "### 다층 퍼셉트론(Multi-Layer Perceptron) \n",
    "⁻   퍼셉트론의 한계\n",
    "⁻   특징 공간 변환을 이용한 다층 퍼셉트론\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 인공신경망 Artificial Neural Network\n",
    "### 사람의 뉴런\n",
    "⁻   두뇌의 가장 작은 정보처리 단위\n",
    "⁻   수상돌기dendrite는 신호 수신\n",
    "⁻   세포체는cell body 간단한 연산\n",
    "⁻   축삭은axon 처리 결과를 전송\n",
    "⁻ 사람은 1011개 정도의 뉴런을 가지며, 뉴런 \n",
    "은 1000개 가량 다른 뉴런과 연결되어 있어 \n",
    "1014개 정도의 연결\n",
    "### 인공신경망\n",
    "⁻   기계 학습 역사에서 가장 오래된 기계 학습 \n",
    "모델이며, 현재 가장 다양한 형태를 가짐\n",
    "⁻   딥 러닝의 기초\n",
    "### 인공신경망의  간략한  역사\n",
    "•  1943년 매컬럭과 피츠의 최초의 신경망\n",
    "•  1949년 헤브는 최초로 학습 알고리즘 제안\n",
    "•  1958년 로젠블렛은 퍼셉트론 제안\n",
    "•  위드로와 호프의 Adaline 과 Madaline\n",
    "•  1960년대의 과대 평가\n",
    "•  1969년 민스키와 페퍼트의 저서 『Perceptrons』는 퍼셉트론의 한계를 수학적으로 입증 \n",
    "⁻    퍼셉트론은 선형분류기에 불과하여 XOR 문제조차 해결 못함\n",
    "•  신경망 연구 퇴조\n",
    "•  1986년 루멜하트의 저서 『Parallel Distributed Processing』은 다층 퍼셉트론 제안\n",
    "•  신경망 연구 부활\n",
    "•  1990년대 SVM에 밀리는 형국\n",
    "•  2010년대 딥러닝이 실현되어 신경망이 기계 학습의 주류 기술로 자리매김\n",
    "### 인공신경망의  특징\n",
    "• 학습이 가능함\n",
    "⁻   데이터만 주어지면 신경망은 예제로부터 배울 수 있음\n",
    "• 몇 개의 소자가 오작동 하더라도 전체적으로는 큰 문제가 발생하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론 perceptron (기본신경망 구조)\n",
    "• 퍼셉트론 (입력을 여러개 받아 출력함)\n",
    "⁻   1957년 로젠블라트(Frank Rosenblatt) \n",
    "⁻   가장 원시적 신경망\n",
    "⁻   생물학적 뉴런을 수학적인 모델로 만듦\n",
    "\n",
    "⁻   노드, 가중치, 층과 같은 새로운 개념을 도입하고 학습 알고리즘을 창안함\n",
    "⁻   딥러닝을 포함한 현대 신경망은 퍼셉트론을 병렬과 순차 구조로 결합하여 만듦 \n",
    "→ 현대 신경망의 중요한 구성 요소\n",
    "용어(가중치 등) + 이해가 중요함 \n",
    "### 퍼셉트론의  구조\n",
    "• 입력층과 출력층을 가짐\n",
    "⁻ 입력층은 연산을 하지 않으므로 퍼셉트론은 단일 층 구조라고 간주 \n",
    "->Single-layer Perceptron\n",
    "• 입력층의 i번째 노드는 특징 벡터 𝐱𝐱 = 𝑥𝑥1 , 𝑥𝑥2 , ⋯ , 𝑥𝑥𝑑𝑑 T 의 요소 𝑥𝑥𝑖𝑖 를 담당\n",
    "• 항상 1이 입력되는 바이어스 노드 -> 그림에서 입력층 첫번째 노드 \n",
    "• 출력층은 한 개의 노드\n",
    "• i번째 입력층 노드와 출력층을 연결하는 에지는 가중치 𝑤𝑤𝑖𝑖 를 가짐\n",
    "\n",
    "### 퍼셉트론의  동작\n",
    "활성화 함수\n",
    "Activation function = 계단 함수(Python)\n",
    "• 뉴런에서는 입력 신호의 가중치 합이 어떤 임계값을 넘는 경우에만 뉴런이 활성 \n",
    "화되어서 1을 출력, 그렇지 않으면 0을 출력\n",
    "\n",
    "\n",
    "#### • 2차원 특징 벡터로 표현되는 샘플을 4개 가진 훈련 집합 (논리 연산 OR)\n",
    "#그림\n",
    "• 아래의 퍼셉트론은 위의 데이터를 분류하는가?\n",
    "#그림1,2\n",
    "\n",
    "#### • 퍼셉트론을 기하학적으로 설명하면\n",
    "• 𝑤𝑤1 과 𝑤𝑤2 는 직선의 방향, 𝑤𝑤0 은 절편을 결정\n",
    "• 결정 직선은 전체 공간을 +1과 -1의 두 부분공간으로 분할하는 분류기 역할\n",
    "\n",
    "⁻   d 차원 공간에서는\n",
    "• 2차원은 결정 직선, 3차원은 결정 평면, 4차원 이상은 결정 초평면\n",
    "\n",
    "퍼셉트론에서  학습이란?\n",
    "• 지금까지는 학습을 마친 퍼셉트론을 가지고 동작을 설명한 셈\n",
    "#### 중요포인트 ->  𝑤𝑤0 ,  𝑤𝑤1 ,  𝑤𝑤2 가 어떤 값을 가져야 100% 옳게 분류할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고민해보기, [실습] 퍼셉트론을  이용한  AND 연산  구현\n",
    "• 적절한 w0, w1, w2 값은 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론  학습  알고리즘 x-> 입력 d-> 정답 y-> 예측값\n",
    "Input: 학습 데이터(x1, d1), …, (x^m, d^m)\n",
    "1.  모든 가중치(w)들과 바이어스(b)를 0 또는 작은 난수로 초기화\n",
    "2.  while (가중치가 변경되지 않을 때까지 반복, 또는 일정 횟수, 또는 일정 오차 이하 등)\n",
    "3.        각 학습 데이터 x^k와 정답 d^k에 대하여\n",
    "4.              y^k(t) = f(w•x)\n",
    "5.               if d^k==y^k(t)\n",
    "6.                     continue\n",
    "7.              else -> 가중치를 업데이트 해줘라\n",
    "8.                  모든 가중치 wi에 대하여 wi(t+1) = wi(t) + α•(d^k-y^k(t))•xi^k\n",
    "                                           #가중치             #오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론의  AND 연산  학습  과정\n",
    "• w=0, b=0, 학습률(α)=0.1                   wi(t+1) = wi(t) + α•(dk-yk(t))•xik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론  프로그래밍\n",
    "# 뉴론의 출력 계산 함수  \n",
    "def calculate(input):\n",
    "    global weights \n",
    "    global bias\n",
    "    activation = bias    # 바이어스\n",
    "    for i in range(2):    # 입력신호 총합 계산 \n",
    "        activation += weights[i] * input[i]\n",
    "    if activation >= 0.0:    # 스텝 활성화 함수 \n",
    "        return 1.0 \n",
    "    else: \n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습  알고리즘 \n",
    "def train_weights(X, y, l_rate, n_epoch): \n",
    "    global weights\n",
    "    global bias\n",
    "    for epoch in range(n_epoch):   # 에포크  반복 \n",
    "        sum_error = 0.0\n",
    "        for row, target in zip(X, y):  # 반복\n",
    "            actual = calculate(row)    # 실제  출력  계산 \n",
    "            error = target - actual      # 오류  계산 \n",
    "            bias = bias + l_rate * error\n",
    "            sum_error += error**2      # 오류  제곱  계산 \n",
    "            for i in range(2):              # 가중치  변경\n",
    "                weights[i] = weights[i] + l_rate * error * row[i]\n",
    "                print(weights, bias)\n",
    "            print('에포크  번호=%d, 학습률=%.3f, 오류=%.3f' % (epoch, l_rate, sum_error)) \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0] -0.1\n",
      "[0.0, 0.0] -0.1\n",
      "에포크  번호=0, 학습률=0.100, 오류=1.000\n",
      "[0.0, 0.0] -0.1\n",
      "[0.0, 0.0] -0.1\n",
      "에포크  번호=0, 학습률=0.100, 오류=1.000\n",
      "[0.0, 0.0] -0.1\n",
      "[0.0, 0.0] -0.1\n",
      "에포크  번호=0, 학습률=0.100, 오류=1.000\n",
      "[0.1, 0.0] 0.0\n",
      "[0.1, 0.1] 0.0\n",
      "에포크  번호=0, 학습률=0.100, 오류=2.000\n",
      "[0.1, 0.1] 0.0\n"
     ]
    }
   ],
   "source": [
    "# AND 연산 학습 데이터셋, 샘플과 레이블이다. \n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y = [0, 0, 0, 1]\n",
    "# 가중치와 바이어스 초기값 \n",
    "weights = [0.0, 0.0] \n",
    "bias = 0.0\n",
    "l_rate = 0.1                # 학습률\n",
    "n_epoch = 5                # 에포크 횟수\n",
    "weights = train_weights(X, y, l_rate, n_epoch) \n",
    "print(weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# sklearn으로  퍼셉트론  실습하기\n",
    "from sklearn.linear_model import Perceptron \n",
    "\n",
    "# AND 연산 샘플과 레이블이다.\n",
    "X = [[0,0],[0,1],[1,0],[1,1]] \n",
    "y = [0, 0, 0, 1]\n",
    "# 퍼셉트론 생성. tol는 종료 조건, random_state는 난수의 시드 \n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "# 학습을 수행한다. \n",
    "clf.fit(X, y)\n",
    "\n",
    "# 테스트를 수행한다. \n",
    "print(clf.predict(X)) # 실제 출력값 \n",
    "# 정확도 테스트 \n",
    "print(clf.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 0 1 1 2 0 2 2 0 0 2 2 0 0 2 0 0 1 1 0 2 2 0 2 2 2 0\n",
      " 2 0 1 2 0 2 0 0]\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# [실습]Perceptron를 이용한 iris 데이터 분류\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# iris.data, iris.target에 데이터와 레이블 정보가 저장되어 있음 \n",
    "iris = load_iris()\n",
    "\n",
    "# 학습, 테스트 데이터 분할 7:3\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, \n",
    "random_state=0)\n",
    "clf = Perceptron(tol=1e-3, random_state=0) \n",
    "\n",
    "# 학습을 수행한다.\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "# 테스트를 수행한다.\n",
    "print(clf.predict(X_test))\n",
    "\n",
    "# 정확도가 얼마 나오는지 확인해보자! \n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 퍼셉트론의  한계\n",
    "• 퍼셉트론은 선형 분류기라는 한계\n",
    "⁻   선형 분리 불가능한 상황에서는 일정한 양의 오류 \n",
    "⁻   예) XOR 문제에서는 75%의 정확도 한계\n",
    "\n",
    "⁻   민스키의 『Perceptrons』\n",
    "• 퍼셉트론의 한계를 지적하고 다층 구조를 이용한 극복 방안 제시. 당시 기술로 실현 불가능\n",
    "• 1974년 웨어보스는 박사 논문에서 오류 역전파 알고리즘 제안\n",
    "• 1986년 룸멜하트의 저서 『Parallel Distributed Processing』 다층 퍼셉트론 이론 정립하여 신경망 부활"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 퍼셉트론의  XOR 데이터  분류\n",
    "from sklearn.linear_model import Perceptron \n",
    "\n",
    "# XOR 연산 샘플과 레이블이다.\n",
    "X = [[0,0],[0,1],[1,0],[1,1]] \n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# 퍼셉트론 생성. tol는 종료 조건, random_state는 난수의 시드 \n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "\n",
    "# 학습을 수행한다. \n",
    "clf.fit(X, y)\n",
    "\n",
    "# 테스트를 수행한다. \n",
    "print(clf.predict(X))\n",
    "# 정확도 테스트 \n",
    "print(clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론  2개를  사용한  XOR 문제의  해결\n",
    "• 퍼셉트론 ①과 퍼셉트론 ②가 모두 +1이면 ●, 그렇지 않으면 □로 분류\n",
    "\n",
    "\n",
    "### 특징  공간  변환\n",
    "• 퍼셉트론 2개를 병렬로 결합하면,\n",
    "⁻ 원래 공간 𝐱𝐱  =   𝑥𝑥1 , 𝑥𝑥2     T 를 새로운 특징 공간 𝐳𝐳  =  𝑧𝑧1 , 𝑧𝑧2     T 로 변환 \n",
    "⁻ 새로운 특징 공간 𝐳𝐳에서는 선형 분리 가능함\n",
    "\n",
    "• 퍼셉트론 1개를 순차 결합하면?\n",
    "⁻ 새로운 특징 공간 𝐳𝐳에서 선형 분리를 수행하는 퍼셉트론 ③을 순차 결합하면 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akkn9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 다층  퍼셉트론의  XOR 데이터  분류\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "# XOR 연산 샘플과 레이블이다.\n",
    "X = [[0,0],[0,1],[1,0],[1,1]] \n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# 다층 퍼셉트론 생성.\n",
    "# 다양한 매개변수(hidden_layer_sizes, activation, solver, learning_rate_init 등)를 이용해 MLP 구성 \n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# 학습을 수행한다. \n",
    "mlp.fit(X, y)\n",
    "\n",
    "# 테스트를 수행한다. \n",
    "print(mlp.predict(X))\n",
    "# 정확도 테스트 \n",
    "print(mlp.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
