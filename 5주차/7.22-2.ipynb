{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론과 심층 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목차 \n",
    "### •  다층 퍼셉트론 Multi-Layer Perceptron \n",
    "⁻   다층 퍼셉트론의 구조\n",
    "⁻   은닉층\n",
    "⁻   활성화 함수\n",
    "⁻   오차 역전파 알고리즘\n",
    "• 다층 퍼셉트론 구현\n",
    "⁻   딥러닝 프레임워크\n",
    "⁻   Tensorflow를 이용한 MLP 구현\n",
    "### • 심층 신경망 Deep Neural Network \n",
    "⁻   딥러닝 성공 요인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 들어가기전에.. \n",
    "#### Input layer(입력층) : 입력값(Raw ddata : 가공되지 않은 측정 자료)을 받습니다.\n",
    "#### Hidden layer(은닉층) : 데이터를 처리하고 신경망ㅇ이 수행하도록 설계된 작업을 합니다.\n",
    "#### Output layer(출력층) : 처리된 데이터를 기반으로 예측 또는 결정을 내립니다.\n",
    "\n",
    "### 다층  퍼셉트론  MLP(Multi-Layer Perceptron)\n",
    "• 입력층과 출력층 사이에 은닉층(hidden layer)을 가지고 있는 신경망 \n",
    "⁻   퍼셉트론을 병렬과 순차 구조로 결합한 구조\n",
    "⁻   완전 연결(fully-connected) 구조 \n",
    "⁻   층 수 = 은닉층 수 + 출력층 1개\n",
    "\n",
    "• 다층 퍼셉트론의 핵심 아이디어 \n",
    "⁻   은닉층 사용\n",
    "⁻   시그모이드 활성화 함수 사용 -> 전에는 계단 함수였는데..\n",
    "⁻   오차 역전파 학습 알고리즘 -> 학습의 난이도 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 은닉층  Hidden Layer\n",
    "• 은닉층은 특징 추출기\n",
    "⁻ 은닉층은 특징 벡터를 분류에 더 유리한 새로운 특징 공간으로 변환 \n",
    "⁻ 현대 기계 학습에서는 특징 학습이라feature learning 부름\n",
    "(딥러닝은 더 많은 단계를 거쳐 특징 학습을 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 은닉층의  구조는  어떻게?\n",
    "• 호닉의 주장[Hornik1989]\n",
    "\n",
    "\n",
    "⁻   은닉 노드를 무수히 많게 할 수 없으므로, 실질적으로는 복잡한 구조의 데이터에서는 성능 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화  함수  Activation Function\n",
    "• 단층 퍼셉트론의 활성화 함수\n",
    "⁻   “퍼셉트론에서는 활성화 함수로 계단함수를 이용한다.”\n",
    "⁻   계단 함수(step function): 임계값을 경계로 출력이 바뀌는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시그모이드  함수와  계단  함수  비교\n",
    "• 차이점\n",
    "1. 시그모이드 함수는 부드러운 곡선이며 입력에 따 라 출력이 연속적으로 변화, 계단 함수는 0을 경계로 출력이 갑자기 바뀜\n",
    "2. 시그모이드 함수는 실수, 계단 함수는 0과 1 중 하나의 값만 반환\n",
    "\n",
    "• 공통점\n",
    "1.   입력이 작을 때의 출력은 0에 가깝고, 입력이 커지 \n",
    "면 출력이 1에 가까워짐\n",
    "2.   입력이 아무리 작거나 커도 출력은 0~1 사이 값\n",
    "3.   비선형 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화  함수  Activation Function\n",
    "• 미분 가능하고 연속적인 함수를 사용\n",
    "⁻   학습 알고리즘이 활성화 함수의 일차 미분 값을 사용하기 때문\n",
    "\n",
    "* 다양한 활성화 함수 및 장단점 정리 \n",
    "- https://nittaku.tistory.com/267\n",
    "- https://gooopy.tistory.com/51?category=824281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화  함수  Activation Function\n",
    "• 신경망이 사용하는 다양한 활성화 함수\n",
    "⁻   로지스틱 시그모이드와 하이퍼볼릭 탄젠트는 a가 커질수록 계단함수에 가까워짐 \n",
    "⁻   모두 1차 도함수 계산이 빠름 (특히 ReLU는 비교 연산 한 번)\n",
    "\n",
    "⁻   주로 퍼셉트론은 계단함수, 다층 퍼셉트론은 로지스틱 시그모이드와 하이퍼볼릭 탄젠트, \n",
    "딥러닝은 ReLU를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층  퍼셉트론의  동작\n",
    "• 순방향 전파(feed-forward)를 통해 출력층에서 계산되어 나오는 값을 사용 \n",
    "⁻   각 노드에서의 출력 값은 퍼셉트론의 동작 방법과 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순방향  출력  계산\n",
    "current = input\n",
    "for each layer in network \n",
    "for each neuron i in layer\n",
    "net = get_sum(weights[i], current)  # 입력의 가중합 계산 \n",
    "output[i] = activation_func(net)    # 각 노드들의 출력 계산\n",
    "current = output                    # 다음 계층은 현재 계층의 출력을 입력으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층  퍼셉트론의  학습 -> \n",
    "• 다층 퍼셉트론에서 순방향 학습이 가능한가?\n",
    "• 오차 역전파(error back-propagation) 알고리즘\n",
    "⁻   입력이 주어지면 순방향으로 계산하여 출력을 계산한 후에 실제 출력과 우리가 원하는 출력 간의 \n",
    "오차를 계산\n",
    "⁻   이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오차  역전파  알고리즘  의사  코드\n",
    "신경망의 가중치를 작은 난수로 초기화한다.\n",
    "do 각 훈련 샘플 sample에 대하여 다음을 반복한다. \n",
    "    actual = calculate_network(sample)  // 순방향 패스 \n",
    "    target = desired_output(sample)\n",
    "    각 출력 노드에서 오차(target - actual)을 계산한다.\n",
    "    은닉층에서 출력층으로의 가중치 변경값을 계산한다. // 역방향 패스 \n",
    "    입력층에서 은닉층으로의 가중치 변경값을 계산한다. // 역방향 패스 \n",
    "    전체 가중치를 업데이트한다.\n",
    "until 모든 샘플이 올바르게 분류될 때까지 -> 끝나지 않을 수도.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실  함수 -> (작을수록 좋다.) 손실을 낮추는게 *학습목표\n",
    "#### • 지금 당신은 얼마나 행복한가요? \n",
    "⁻   내 행복지수는 10.23입니다.\n",
    "⁻   행복함의 정도를 행복지수라는 하나의 지표로 표현\n",
    "#### • 손실 함수(loss function)\n",
    "⁻   신경망 학습에서는 현재의 상태(성능)를 ‘하나의 지표’, 손실 함수로 표현\n",
    "⁻   신경망 학습: 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것 \n",
    "⁻   손실 함수는 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는지를 표현 \n",
    "⁻   손실 함수는 일반적으로 평균 제곱 오차와 교차 엔트로피 오차 사용\n",
    "#### • ‘정확도’ 대신 ‘손실 함수의 값’을 지표로 사용하는 이유는? \n",
    "⁻   정확도는 가중치의 미세한 변화에는 거의 반응을 보이지 않음\n",
    "⁻ 손실 함수는 매개변수의 값이 조금 변하면 그에 반응하여 손실 함수의 값도 연속적으로 변화함\n",
    "• 손실 함수 값의 변화에 따라 적절하게 매개변수를 갱신할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사  하강법 손실함수 -> 가중치, 손실함수\n",
    "• 정확한 기울기를 계산하려면 전체 데이터에 대해서 연산을 해야 해서 효율성에 \n",
    "대한 문제 발생\n",
    "\n",
    "⁻   전체 데이터가 아니라 하나의 샘플에서 비용함수의 경사를 계산하여 경사하강법을 사용해도 대체 \n",
    "로 괜찮은 결과를 얻을 수 있다는 것이 경험적으로 입증\n",
    "= 확률적 경사하강법(stochastic gradient descent, SGD) \n",
    "\n",
    "⁻   실제로는 하나의 샘플만을 사용하지는 않고, 전체 데이터 중 10개에서 1,000개 정도의 샘플이 포 \n",
    "함된 배치를 사용\n",
    "= 미니 배치 경사하강법(mini-batch gradient descent)\n",
    "\n",
    "w= w(가중치)− α(학습률) = 𝜕𝐿/𝜕𝑤 (기울기)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
